{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no: 1\n",
      "----------\n",
      "Train Error: 0.06739618096746113 \n",
      " Train Accuracy: 84.86162054018006 \n",
      " Test Error: 0.07030701390638902 \n",
      " \n",
      " \n",
      "\n",
      "Epoch no: 2\n",
      "----------\n",
      "Train Error: 0.06693762048427658 \n",
      " Train Accuracy: 84.86162054018006 \n",
      " Test Error: 0.06982803172113149 \n",
      " \n",
      " \n",
      "\n",
      "Epoch no: 3\n",
      "----------\n",
      "Train Error: 0.06647058434094603 \n",
      " Train Accuracy: 84.86162054018006 \n",
      " Test Error: 0.06933757546783556 \n",
      " \n",
      " \n",
      "\n",
      "Epoch no: 4\n",
      "----------\n",
      "Train Error: 0.06600264287794506 \n",
      " Train Accuracy: 84.86162054018006 \n",
      " Test Error: 0.06884309090916127 \n",
      " \n",
      " \n",
      "\n",
      "Epoch no: 5\n",
      "----------\n",
      "Train Error: 0.0655427208481102 \n",
      " Train Accuracy: 84.86162054018006 \n",
      " Test Error: 0.06835350487492009 \n",
      " \n",
      " \n",
      "\n",
      "Epoch no: 6\n",
      "----------\n",
      "Train Error: 0.06510052452248943 \n",
      " Train Accuracy: 84.86162054018006 \n",
      " Test Error: 0.06787869048493433 \n",
      " \n",
      " \n",
      "\n",
      "Epoch no: 7\n",
      "----------\n",
      "Train Error: 0.06468567843958513 \n",
      " Train Accuracy: 84.86162054018006 \n",
      " Test Error: 0.06742862962191878 \n",
      " \n",
      " \n",
      "\n",
      "Epoch no: 8\n",
      "----------\n",
      "Train Error: 0.06430668585081203 \n",
      " Train Accuracy: 84.86162054018006 \n",
      " Test Error: 0.06701237625144567 \n",
      " \n",
      " \n",
      "\n",
      "Epoch no: 9\n",
      "----------\n",
      "Train Error: 0.06396992215519104 \n",
      " Train Accuracy: 84.86162054018006 \n",
      " Test Error: 0.0666370224754779 \n",
      " \n",
      " \n",
      "\n",
      "Epoch no: 10\n",
      "----------\n",
      "Train Error: 0.06367890927557085 \n",
      " Train Accuracy: 84.86162054018006 \n",
      " Test Error: 0.06630691501486041 \n",
      " \n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST Accuracy:  84.26666666666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "from helperFunctions import *\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.target = None\n",
    "        self.features = None\n",
    "        self.data = None\n",
    "        self.testX = None\n",
    "        self.testY = None\n",
    "        self.trainX = None\n",
    "        self.trainY = None\n",
    "        \n",
    "    def _readDataset(self, filename):\n",
    "        self.data = pd.read_csv(filename)\n",
    "\n",
    "    def _dropNulls(self):\n",
    "        self.data.drop([\"education\"], axis = 1, inplace = True) #dropping this improved accuracy\n",
    "        self.data.dropna(inplace = True)\n",
    "    \n",
    "    def _saveProcessedData(self):\n",
    "        self.data = self.data.apply(lambda x: normalize(x))\n",
    "        self.features = self.data.drop(\"TenYearCHD\", axis = 1)\n",
    "        self.target = self.data.TenYearCHD\n",
    "        self.data.to_csv(\"../data/processedData.csv\")\n",
    "    \n",
    "    def _trainTestSplit(self):\n",
    "        self.trainX, self.testX, self.trainY, self.testY = train_test_split(self.features, self.target, test_size=0.2)\n",
    "        return self.trainX, self.testX, self.trainY, self.testY\n",
    "\n",
    "    def preProcessing(self, filename):\n",
    "        self._readDataset(filename)\n",
    "        self._dropNulls()\n",
    "        self.data.reset_index(drop = True)\n",
    "        self._saveProcessedData()\n",
    "        self._trainTestSplit()  \n",
    "      \n",
    "model = Model()\n",
    "model.preProcessing(\"../data/framingham.csv\")\n",
    "trainX, testX, trainY, testY = model._trainTestSplit()\n",
    "trainY = np.array(trainY).reshape(-1, 1)\n",
    "train = [trainX, trainY]\n",
    "testY = np.array(testY).reshape(-1, 1)\n",
    "\n",
    "test = [testX, testY]\n",
    "trainError = []\n",
    "trainAccuracy = []\n",
    "testError = []\n",
    "\n",
    "\n",
    "class MyNeuralNet:\n",
    "    def __init__(self, layerArray):\n",
    "        \n",
    "        \"\"\" \n",
    "        layerArray: \n",
    "            array of dimensions of layers. \n",
    "            size of layerArray is the number of layers in our network\n",
    "        \"\"\"\n",
    "        trainError = []\n",
    "        trainAccuracy = []\n",
    "        testError = []\n",
    "\n",
    "\n",
    "        self.layers = layerArray #layes in our network\n",
    "        self.B = [] #bias matrix\n",
    "        self.W = [] #weights matrix\n",
    "        self.input = None \n",
    "\n",
    "        for layerNum in range(1, len(layerArray)): #1st layer is input so we exclude that\n",
    "            biasVector = np.zeros((layerArray[layerNum], 1)) #bias zero initialized \n",
    "            self.B.append(biasVector)\n",
    "            weightsMatrix = np.random.normal(loc = 0, scale = 1, size = (layerArray[layerNum], layerArray[layerNum-1])) #weights initialized with normal dist\n",
    "            self.W.append(weightsMatrix)\n",
    "\n",
    "    \n",
    "    def netSize(self):\n",
    "        \"\"\" \n",
    "        number of layers in the network excluding the input layer\n",
    "        \"\"\"    \n",
    "        return len(self.layers) - 1\n",
    "    \n",
    "    def activateLayer(self, z):\n",
    "        \"\"\"\n",
    "        applies activation function to the layer z. \n",
    "        activation : sigmiod\n",
    "        \"\"\"\n",
    "        activatedLayer = sigmoid(z)\n",
    "        return activatedLayer\n",
    "    \n",
    "    def derivatieActivateLayer(self, z):\n",
    "        \"\"\" \n",
    "        applies derivate of activation function to the layer z. \n",
    "        activation : sigmiod\n",
    "        \"\"\"\n",
    "        z = np.array(z)\n",
    "        sigmoid = self.activateLayer(z)\n",
    "        return sigmoid*(1-sigmoid)\n",
    "\n",
    "\n",
    "    def forwardPass(self, layer):\n",
    "        \"\"\"\n",
    "        passes through the network, calculates linear score and then applies sigmoid to it.\n",
    "        \"\"\"\n",
    "        for i in range(self.netSize()):\n",
    "            layer = np.dot(layer, self.W[i].T) + self.B[i].T\n",
    "            layer = self.activateLayer(layer)\n",
    "        return layer\n",
    "    \n",
    "    def backPropagate(self, x, y):\n",
    "        \"\"\"\n",
    "        Backpropagates through the network to calculate gradients. \n",
    "        \"\"\"\n",
    "\n",
    "        #dW and dB hold the gradients of cost wrt. weights and biases. initilially zero\n",
    "        dW = []\n",
    "        dB = []\n",
    "        for i in range(self.netSize()):\n",
    "            dW.append(np.zeros(self.W[i].shape))\n",
    "            dB.append(np.zeros(self.B[i].shape))\n",
    "        #(10, 14) (1, 10)\n",
    "\n",
    "        outputLayers = [] #Z's\n",
    "        activeOutputLayers = [] #Sigmoid of Z's or g(Z)\n",
    "        n = self.netSize()\n",
    "        activeOutput = x #input layer \n",
    "        activeOutputLayers.append(activeOutput)\n",
    "        \n",
    "        for b,w in zip(self.B, self.W):\n",
    "            output = np.dot(w, activeOutput) + b\n",
    "            outputLayers.append(output)\n",
    "            activeOutput = self.activateLayer(output)\n",
    "            activeOutputLayers.append(activeOutput)\n",
    "        \n",
    "        outputLayers = np.array(outputLayers)\n",
    "        activeOutputLayers = np.array(activeOutputLayers)\n",
    "        \n",
    "\n",
    "        dZ = derivateCost(activeOutput, y) * self.derivatieActivateLayer(output)\n",
    "        dW[n-1] = np.dot(dZ, activeOutputLayers[-2].T)\n",
    "        dB[n-1] = dZ\n",
    "\n",
    "        for l in range(n-1):\n",
    "            dZ = np.dot(self.W[n-1-l].T, dZ) * self.derivatieActivateLayer(outputLayers[n-2-l])\n",
    "            dB[l] = dZ\n",
    "            dW[l] = np.dot(dZ , activeOutputLayers[max(0,n-3-l)].T)\n",
    "\n",
    "        return (np.array(dB), np.array(dW))\n",
    "   \n",
    "\n",
    "    def train(self, train, test, epochs, batchSize, learningRate, validation = None):\n",
    "        \"\"\"\n",
    "        Trains the network using mini-batch gradient descent.\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(epochs):\n",
    "            for batch in dataIter(batchSize, train):\n",
    "                xBatch, yBatch = batch[0], batch[1]\n",
    "                dW = []\n",
    "                dB = []\n",
    "                #print(xBatch)\n",
    "\n",
    "                #initialize gradients\n",
    "                for j in range(self.netSize()):\n",
    "                    dW.append(np.zeros(self.W[j].shape))\n",
    "                    dB.append(np.zeros(self.B[j].shape))\n",
    "                \n",
    "                for x, y in zip(xBatch, yBatch):\n",
    "                    x = x.reshape(14, 1)\n",
    "                    #obtain gradients by backpropagating\n",
    "                    gradB, gradW = self.backPropagate(x, y)\n",
    "                    n = self.netSize()\n",
    "\n",
    "                    #summing weights and biases for all examples in the mini batch\n",
    "                    dW = [w + gradw for w, gradw in zip(dW, gradW)]\n",
    "                    dB = [b + gradb for b, gradb in zip(dB, gradB)]\n",
    "\n",
    "                for j in range(self.netSize()):\n",
    "                    self.W[j] = self.W[j] - (learningRate/batchSize)*dW[j]\n",
    "                    self.B[j] = self.B[j] - (learningRate/batchSize)*dB[j]\n",
    "            \n",
    "            trainError.append(calculateError(self, train))\n",
    "            trainAccuracy.append(accuracy(self.forwardPass(train[0]), train[1]))\n",
    "            testError.append(calculateError(self, test))\n",
    "\n",
    "            print(\"Epoch no: {}\\n----------\".format(i+1))\n",
    "            print(\"Train Error: {0} \\n Train Accuracy: {1} \\n Test Error: {2} \\n \\n \\n\".format(trainError[i], trainAccuracy[i], testError[i]))\n",
    "        plt.plot(range(epochs), trainError, label = 'Train Error')\n",
    "        plt.plot(range(epochs), testError, label = 'Test Error')\n",
    "        plt.legend()\n",
    "        plt.title(\"Train and Test loss vs epochs\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(range(epochs), trainAccuracy, label = 'Train Accuracy')\n",
    "        plt.legend()\n",
    "        plt.title(\"Train accuracy vs epochs\")\n",
    "        #plt.show()\n",
    "        \n",
    "        print(\"TEST Accuracy: \", accuracy(self.forwardPass(test[0]), test[1]))\n",
    "\n",
    "network = MyNeuralNet([train[0].shape[1], 10, 1])\n",
    "network.train(train, test, 10, 10, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
