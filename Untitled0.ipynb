{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sNpiEuJXJsod"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "#sigmoid(z)\n",
    "def sigmoid(z): \n",
    "    return 1/(1+np.exp(-z)) \n",
    "\n",
    "#MSE\n",
    "def cost(yPred, y): \n",
    "    yPred, y = np.array(yPred), np.array(y)\n",
    "    return 0.5*(yPred-y)**2\n",
    "\n",
    "#derivative of cost \n",
    "def derivateCost(yPred, y): \n",
    "    return yPred - y\n",
    "\n",
    "#iterator\n",
    "def dataIter(batchSize, data):\n",
    "    #random.shuffle(data)\n",
    "    batches = []\n",
    "\n",
    "    for i in range(0, np.array(data[0]).shape[0], batchSize):\n",
    "        batchX, batchY = np.array(data[0][i:i+batchSize]), np.array(data[1][i:i+batchSize])\n",
    "        batches.append([batchX, batchY])\n",
    "    return batches\n",
    "\n",
    "#accuracy\n",
    "def accuracy(predicted, actual):\n",
    "    predicted = predicted > 0.5\n",
    "    actual = np.array(actual).reshape(actual.shape[0])\n",
    "    predicted = np.array(predicted).reshape(predicted.shape[0])\n",
    "    \n",
    "    acc = (np.sum(actual == predicted)/len(actual)) * 100\n",
    "    return acc\n",
    "\n",
    "#normalize, scale btw 0 and 1\n",
    "def normalize(x):\n",
    "    x = x - np.amin(x)\n",
    "    x = x/np.amax(x)\n",
    "    return x\n",
    "\n",
    "#calculates error of entire batch.\n",
    "def calculateError(net, dataset):\n",
    "    j = 0\n",
    "    yPred = net.forwardPass(dataset[0])\n",
    "    y = dataset[1]\n",
    "    costEx = cost(yPred, y) #error of single example.\n",
    "    j = costEx.sum()\n",
    "    return j/len(dataset[0]) #taking mean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "GNyJUg81HBCw",
    "outputId": "0e8177d9-71e6-44fc-a2ad-6a80d3e748d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 784) (30, 1) (32, 1470000, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (30,784) and (32,1470000,1) not aligned: 784 (dim 1) != 1470000 (dim 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-87787967435c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyNeuralNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.03\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-87787967435c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train, test, epochs, batchSize, learningRate, validation)\u001b[0m\n\u001b[1;32m    152\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mtrainError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0mtrainAccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardPass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mtestError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-6ead3087d032>\u001b[0m in \u001b[0;36mcalculateError\u001b[0;34m(net, dataset)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0myPred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardPass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mcostEx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myPred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#error of single example.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-87787967435c>\u001b[0m in \u001b[0;36mforwardPass\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivateLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (30,784) and (32,1470000,1) not aligned: 784 (dim 1) != 1470000 (dim 1)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "#from helperFunctions import *\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist \n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "trainY = np.array(y_train).reshape(-1, 1)\n",
    "x_train = np.array(x_train).reshape(60000, 28*28)\n",
    "train = [x_train, trainY]\n",
    "testY = np.array(y_test).reshape(-1, 1)\n",
    "\n",
    "test = [x_test, testY]\n",
    "trainError = []\n",
    "trainAccuracy = []\n",
    "testError = []\n",
    "\n",
    "batchSize = 32\n",
    "class MyNeuralNet:\n",
    "    def __init__(self, layerArray):\n",
    "        \n",
    "        \"\"\" \n",
    "        layerArray: \n",
    "            array of dimensions of layers. \n",
    "            size of layerArray is the number of layers in our network\n",
    "        \"\"\"\n",
    "        trainError = []\n",
    "        trainAccuracy = []\n",
    "        testError = []\n",
    "\n",
    "\n",
    "        self.layers = layerArray #layes in our network\n",
    "        self.B = [] #bias matrix\n",
    "        self.W = [] #weights matrix\n",
    "        self.input = None \n",
    "\n",
    "        for layerNum in range(1, len(layerArray)): #1st layer is input so we exclude that\n",
    "            biasVector = np.zeros((layerArray[layerNum], 1)) #bias zero initialized \n",
    "            self.B.append(biasVector)\n",
    "            weightsMatrix = np.random.normal(loc = 0, scale = 1, size = (layerArray[layerNum], layerArray[layerNum-1])) #weights initialized with normal dist\n",
    "            self.W.append(weightsMatrix)\n",
    "\n",
    "    \n",
    "    def netSize(self):\n",
    "        \"\"\" \n",
    "        number of layers in the network excluding the input layer\n",
    "        \"\"\"    \n",
    "        return len(self.layers) - 1\n",
    "    \n",
    "    def activateLayer(self, z):\n",
    "        \"\"\"\n",
    "        applies activation function to the layer z. \n",
    "        activation : sigmiod\n",
    "        \"\"\"\n",
    "        activatedLayer = sigmoid(z)\n",
    "        return activatedLayer\n",
    "    \n",
    "    def derivatieActivateLayer(self, z):\n",
    "        \"\"\" \n",
    "        applies derivate of activation function to the layer z. \n",
    "        activation : sigmiod\n",
    "        \"\"\"\n",
    "        z = np.array(z)\n",
    "        sigmoid = self.activateLayer(z)\n",
    "        return sigmoid*(1-sigmoid)\n",
    "\n",
    "\n",
    "    def forwardPass(self, layer):\n",
    "        \"\"\"\n",
    "        passes through the network, calculates linear score and then applies sigmoid to it.\n",
    "        \"\"\"\n",
    "        layer = layer.reshape((batchSize, -1, 1))\n",
    "        for i in range(self.netSize()):\n",
    "            print(self.W[i].shape, self.B[i].shape, layer.shape)\n",
    "            layer = np.dot(self.W[i], layer) + self.B[i]\n",
    "            layer = self.activateLayer(layer, func)\n",
    "        return layer\n",
    "    \n",
    "    \n",
    "    def backPropagate(self, x, y):\n",
    "        \"\"\"\n",
    "        Backpropagates through the network to calculate gradients. \n",
    "        \"\"\"\n",
    "\n",
    "        #dW and dB hold the gradients of cost wrt. weights and biases. initilially zero\n",
    "        dW = []\n",
    "        dB = []\n",
    "        for i in range(self.netSize()):\n",
    "            dW.append(np.zeros(self.W[i].shape))\n",
    "            dB.append(np.zeros(self.B[i].shape))\n",
    "        \n",
    "\n",
    "        outputLayers = [] #Z's\n",
    "        activeOutputLayers = [] #Sigmoid of Z's \n",
    "        n = self.netSize()\n",
    "        activeOutput = x #input layer \n",
    "        activeOutputLayers.append(activeOutput)\n",
    "        activeOutput = activeOutput.reshape((-1, 1))\n",
    "        \n",
    "        for b,w in zip(self.B, self.W):\n",
    "            output = np.dot(w, activeOutput) + b\n",
    "            outputLayers.append(output)\n",
    "            activeOutput = self.activateLayer(output)\n",
    "            activeOutputLayers.append(activeOutput)\n",
    "\n",
    "        outputLayers = np.array(outputLayers)\n",
    "        activeOutputLayers = np.array(activeOutputLayers)\n",
    "        n = self.netSize()\n",
    "        dZ = derivateCost(activeOutput, y) * self.derivatieActivateLayer(output)\n",
    "\n",
    "        dW[n-1] = np.dot(dZ, activeOutputLayers[-2].T)\n",
    "        dB[n-1] = dZ\n",
    "        for l in range(2, n):\n",
    "            dZ = np.dot(self.W[-l+1].T, dZ) * self.derivatieActivateLayer(outputLayers[-l])\n",
    "            dB[-l] = dZ\n",
    "            dW[-l] = np.dot(dZ, activeOutputLayers[-l-1].T)\n",
    "\n",
    "        return (np.array(dB), np.array(dW))\n",
    "   \n",
    "\n",
    "    def train(self, train, test, epochs, batchSize, learningRate, validation = None):\n",
    "        \"\"\"\n",
    "        Trains the network using mini-batch gradient descent.\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(epochs):\n",
    "            for batch in dataIter(batchSize, train):\n",
    "                xBatch, yBatch = batch[0], batch[1]\n",
    "                dW = []\n",
    "                dB = []\n",
    "                #print(xBatch)\n",
    "\n",
    "                #initialize gradients\n",
    "                for j in range(self.netSize()):\n",
    "                    dW.append(np.zeros(self.W[j].shape))\n",
    "                    dB.append(np.zeros(self.B[j].shape))\n",
    "\n",
    "                for x, y in zip(xBatch, yBatch):\n",
    "                    #x = x.reshape(14, 1)\n",
    "                    #obtain gradients by backpropagating\n",
    "                    gradB, gradW = self.backPropagate(x, y)\n",
    "                    n = self.netSize()\n",
    "\n",
    "                    #summing weights and biases for all examples in the mini batch\n",
    "                    dW = [w + gradw for w, gradw in zip(dW, gradW)]\n",
    "                    dB = [b + gradb for b, gradb in zip(dB, gradB)]\n",
    "                    break\n",
    "                for j in range(self.netSize()):\n",
    "                    self.W[j] = self.W[j] - (learningRate/batchSize)*dW[j]\n",
    "                    self.B[j] = self.B[j] - (learningRate/batchSize)*dB[j]\n",
    "                break\n",
    "            trainError.append(calculateError(self, train))\n",
    "            trainAccuracy.append(accuracy(self.forwardPass(train[0]), train[1]))\n",
    "            testError.append(calculateError(self, test))\n",
    "\n",
    "            print(\"Epoch no: {}\\n----------\".format(i+1))\n",
    "            print(\"Train Error: {0} \\n Train Accuracy: {1} \\n Test Error: {2} \\n \\n \\n\".format(trainError[i], trainAccuracy[i], testError[i]))\n",
    "        plt.plot(range(epochs), trainError, label = 'Train Error')\n",
    "        plt.plot(range(epochs), testError, label = 'Test Error')\n",
    "        plt.legend()\n",
    "        plt.title(\"Train and Test loss vs epochs\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(range(epochs), trainAccuracy, label = 'Train Accuracy')\n",
    "        plt.legend()\n",
    "        plt.title(\"Train accuracy vs epochs\")\n",
    "        #plt.show()\n",
    "        \n",
    "        print(\"TEST Accuracy: \", accuracy(self.forwardPass(test[0]), test[1]))\n",
    "\n",
    "\n",
    "        \n",
    "network = MyNeuralNet([train[0].shape[1], 30, 10])\n",
    "network.train(train, test, 10, batchSize, 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZryWkd9XHQo6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
